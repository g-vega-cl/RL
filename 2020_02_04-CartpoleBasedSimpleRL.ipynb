{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2020_02_04-CartpoleBasedSimpleRL.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMpXwLP4nCzWOmTmsJ6LLuo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"QmizTEUCXtYs","colab_type":"text"},"source":["#I will try to build an simple environment, and alongside it, a DRL system"]},{"cell_type":"code","metadata":{"id":"ndQijcTYXtqq","colab_type":"code","colab":{}},"source":["from __future__ import absolute_import, division, print_function, unicode_literals\n","\n","# TensorFlow and tf.keras\n","try:\n","  # %tensorflow_version only exists in Colab.\n","  %tensorflow_version 2.x\n","except Exception:\n","  pass\n","import tensorflow as tf\n","from tensorflow import keras\n","import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"l0BPblb9X1Ms","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","import tensorflow.keras.layers as kl\n","import tensorflow.keras.losses as kls\n","import tensorflow.keras.optimizers as ko\n","import logging"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r3ewnw2zYCmA","colab_type":"text"},"source":["#First we need a simple environment.\n","It should take an action, give observations and a reward.\n","I will use my ballon_env\n"]},{"cell_type":"code","metadata":{"id":"XHLdRFu2X_ml","colab_type":"code","colab":{}},"source":["#This will be a simple environment where you only have to take one action to survive.\n","#If you get to 100 points you win\n","class InfiniteEnv():\n","  def __init__(self):\n","    self.action_space = 3 #You can either do A, B, or C.\n","    self.observation_space = np.zeros((3)) #This will literally be random numbers.\n","    self.current_step = -1\n","    self.Death = False\n","    self.totalReward = 0\n","\n","  def _get_reward(self):\n","    \"\"\"Reward is given for staying alive.\"\"\"\n","    if self.Death == False:\n","        return 1\n","    else:\n","        return 0.0\n","\n","  def _get_state(self):\n","    \"\"\"Get the observation.\"\"\"\n","    ob = np.random.rand((3))\n","    return ob        \n","\n","  def _take_action(self, action):\n","    if(action == 1 or action == 2):\n","      self.Death = True\n","    elif(self.totalReward == 100):\n","      self.Death = True\n","    else:\n","      pass\n","\n","  def step(self,action):\n","    if self.Death == True:\n","      raise RuntimeError(\"Episode is done\")\n","    self.curr_step += 1\n","    self._take_action(action)\n","    reward = self._get_reward()\n","    self.totalReward += reward\n","    obs = self._get_state()\n","    return obs, reward, self.Death, {}\n","\n","  def reset(self):\n","    self.curr_step = -1\n","    self.Death = False\n","    return self._get_state()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uJ6VxJ-hYels","colab_type":"code","outputId":"f44aa42c-bd3d-4c5b-e741-59bf77586e73","executionInfo":{"status":"ok","timestamp":1581016986334,"user_tz":360,"elapsed":954,"user":{"displayName":"Cesar vega","photoUrl":"","userId":"04805785551054582010"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["print(np.zeros((3)).shape)\n","print(np.random.rand((3)).shape)"],"execution_count":98,"outputs":[{"output_type":"stream","text":["(3,)\n","(3,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DsBGCExFbWhK","colab_type":"code","colab":{}},"source":["class ProbabilityDistribution(tf.keras.Model):\n","  def call(self, logits, **kwargs):\n","    # Sample a random categorical action from the given logits.\n","    return tf.squeeze(tf.random.categorical(logits,1), axis = -1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UJxUNAxSZNDs","colab_type":"code","colab":{}},"source":["class Model(tf.keras.Model): #what the model is: https://www.tensorflow.org/api_docs/python/tf/keras/Model\n","  def __init__(self, num_actions):\n","    super().__init__(\"mlp_policy\")\n","    self.hidden1 = kl.Dense(128, activation = 'relu')\n","    self.hidden2 = kl.Dense(64, activation= 'relu')\n","    self.value = kl.Dense(1, name = \"value\") \n","    self.logits = kl.Dense(num_actions, name = \"policy_logits\")\n","    self.dist = ProbabilityDistribution()\n","  \n","  def call(self,inputs, **kwargs):\n","    x = tf.convert_to_tensor(inputs) #Inputs son las observations\n","    hidden_logs = self.hidden1(x)\n","    hidden_vals = self.hidden2(x)\n","    return self.logits(hidden_logs), self.value(hidden_vals)\n","  \n","  def action_value(self,obs):\n","    logits, value = self.predict_on_batch(obs) \n","    action = self.dist.predict_on_batch(logits)\n","    return (action, value)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dPeZfDzbYWis","colab_type":"code","colab":{}},"source":["env = InfiniteEnv()\n","env.action_space\n","model = Model(num_actions=env.action_space)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7YZISVnbxVNK","colab_type":"text"},"source":["Si le pasamos solo la (obs) esta tiene una shape 3, lo que hace que todos las dense nos regresen la forma 3, outputShape. Con obs[None] la forma cambia  a 1,3 lo que hace que la dense regrese formas 1, outputshape.\n","print(tf.convert_to_tensor(obs)) #La shape de este es 3,\n","tf.convert_to_tensor(obs[None]) #La shape de este es 1,3"]},{"cell_type":"code","metadata":{"id":"EKq9ZM5xYes6","colab_type":"code","outputId":"0c041beb-5e2a-4ad4-b6ea-8c27c1c0b12d","executionInfo":{"status":"ok","timestamp":1581016986746,"user_tz":360,"elapsed":500,"user":{"displayName":"Cesar vega","photoUrl":"","userId":"04805785551054582010"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["obs = env.reset()\n","action, value = model.action_value(obs[None])\n","print(action, value)"],"execution_count":102,"outputs":[{"output_type":"stream","text":["tf.Tensor([2], shape=(1,), dtype=int64) tf.Tensor([[0.30355552]], shape=(1, 1), dtype=float32)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"PMHFJ_ztxvJH","colab_type":"text"},"source":["Para este punto ya tenemos una red que regresa una acción y un ambiente que regresa lo necesario"]},{"cell_type":"markdown","metadata":{"id":"BNVvH9_FyKV3","colab_type":"text"},"source":["#Now we need to train it\n","We create an agent class to do this."]},{"cell_type":"code","metadata":{"id":"u6LtdAXXZErs","colab_type":"code","colab":{}},"source":["class A2CAgent:\n","  def __init__(self, model, lr=7e-3, gamma=0.99, value_c=0.5, entropy_c=1e-4):\n","    # Coefficients are used for the loss terms.\n","    self.value_c = value_c\n","    self.entropy_c = entropy_c\n","    self.gamma = gamma\n","\n","    self.model = model\n","    self.model.compile(\n","      optimizer=ko.RMSprop(lr=lr),\n","      # Define separate losses for policy logits and value estimate.\n","      loss=[self._logits_loss, self._value_loss])\n","\n","  def test(self, env, render=True):\n","    obs, done, ep_reward = env.reset(), False, 0 #Get the initial values from the environment\n","    while not done:\n","      action, _ = self.model.action_value(obs[None]) #returns the action the model recomends and a value? (What is the value for?)\n","      obs, reward, done, _ = env.step(action)\n","      ep_reward += reward #get the reward of the action\n","    return ep_reward\n","\n","  def _value_loss(self, returns, value):\n","    # Value loss is typically MSE between value estimates and returns.\n","    return self.value_c * kls.mean_squared_error(returns, value)\n","\n","  def _logits_loss(self, actions_and_advantages, logits):\n","    # A trick to input actions and advantages through the same API.\n","    # split that value into 2 ¿through the -1 axis?\n","    actions, advantages = tf.split(actions_and_advantages, 2, axis=-1)\n","\n","    # Sparse categorical CE loss obj that supports sample_weight arg on `call()`.\n","    # `from_logits` argument ensures transformation into normalized probabilities.\n","    weighted_sparse_ce = kls.SparseCategoricalCrossentropy(from_logits=True)\n","\n","    # Policy loss is defined by policy gradients, weighted by advantages.\n","    # Note: we only calculate the loss on the actions we've actually taken.\n","    actions = tf.cast(actions, tf.int32) #transforms into shape tf.int32\n","    policy_loss = weighted_sparse_ce(actions, logits, sample_weight=advantages)\n","\n","    # Entropy loss can be calculated as cross-entropy over itself.\n","    probs = tf.nn.softmax(logits)\n","    entropy_loss = kls.categorical_crossentropy(probs, probs)\n","\n","    # We want to minimize policy and maximize entropy losses.\n","    # Here signs are flipped because the optimizer minimizes.\n","    return policy_loss - self.entropy_c * entropy_loss\n","\n","  def _returns_advantages(self, rewards, dones, values, next_value):\n","    # `next_value` is the bootstrap value estimate of the future state (critic).\n","    #append next_value to np_zeroes_like(rewards) through the -1 axis, and that is returns\n","    returns = np.append(np.zeros_like(rewards), next_value)\n","\n","    # Returns are calculated as discounted sum of future rewards.\n","    for t in reversed(range(rewards.shape[0])): \n","      #bellmans equation\n","      returns[t] = rewards[t] + self.gamma * returns[t + 1] * (1 - dones[t])\n","    returns = returns[:-1] #Take just a single value\n","\n","    # Advantages are equal to returns - baseline (value estimates in our case).\n","    advantages = returns - values\n","    return returns, advantages\n","\n","  def train(self, env, batch_sz=64, updates=250):\n","    # Storage helpers for a single batch of data.\n","    actions = np.empty((batch_sz,), dtype=np.int32)\n","    rewards, dones, values = np.empty((3, batch_sz)) #Create 3 empty arrays of batch_size:\n","    observations = np.empty((batch_sz,) + env.observation_space.shape) #create an array of (batchsize, observation_space.shape)\n","\n","    # Training loop: collect samples, send to optimizer, repeat updates times.\n","    ep_rewards = [0.0] #current episode rewards\n","    next_obs = env.reset() #get observation from the environment\n","    for update in range(updates): #updates is how many times we play the game (our batch count)\n","      for step in range(batch_sz): #run 64 frames to get the batch\n","        observations[step] = next_obs.copy() #set the next_obs in an array of observation\n","        actions[step], values[step] = self.model.action_value(next_obs[None, :]) #pass the obs into the action of our model (the two nnets)\n","        next_obs, rewards[step], dones[step], _ = env.step(actions[step]) #get the next state of the nevironment with the action taken\n","\n","        ep_rewards[-1] += rewards[step]\n","        if dones[step]:  #if we win or loose.\n","          ep_rewards.append(0.0) #start new rewards\n","          next_obs = env.reset() #start new environment\n","          logging.info(\"Episode: %03d, Reward: %03d\" % (\n","            len(ep_rewards) - 1, ep_rewards[-2]))\n","\n","      _, next_value = self.model.action_value(next_obs[None, :]) #after a single batch is finished, run another one before starting the new loop\n","\n","      returns, advs = self._returns_advantages(rewards, dones, values, next_value)\n","      # A trick to input actions and advantages through same API.\n","      acts_and_advs = np.concatenate([actions[:, None], advs[:, None]], axis=-1)\n","\n","      # Performs a full training step on the collected batch.\n","      # Note: no need to mess around with gradients, Keras API handles it.\n","      #observations is the training data, [...the other array...] is the target data.\n","      losses = self.model.train_on_batch(observations, [acts_and_advs, returns])\n","\n","      logging.debug(\"[%d/%d] Losses: %s\" % (update + 1, updates, losses))\n","\n","    return ep_rewards\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hHTVIwn1y9ow","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":72},"outputId":"1a03d616-32dd-48a4-c5bb-d934fcca8d65","executionInfo":{"status":"ok","timestamp":1581017016474,"user_tz":360,"elapsed":28979,"user":{"displayName":"Cesar vega","photoUrl":"","userId":"04805785551054582010"}}},"source":["agent = A2CAgent(model)\n","rewards_history = agent.train(env)\n","print(\"Finished training, testing...\")\n","print(\"%d out of 200\" % agent.test(env)) # 200 out of 200"],"execution_count":104,"outputs":[{"output_type":"stream","text":["train_enter\n","Finished training, testing...\n","0 out of 200\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"F2Cgshd99ePs","colab_type":"text"},"source":["Nota: No se como poner que \"ganaste\" pero debería de incrementar la reward con el training. Para comprobar haré una prueba simple"]},{"cell_type":"code","metadata":{"id":"3Iij9xt8z8B6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"d57adcca-831f-47e0-86eb-8eb52f2d1233","executionInfo":{"status":"ok","timestamp":1581017272102,"user_tz":360,"elapsed":1217,"user":{"displayName":"Cesar vega","photoUrl":"","userId":"04805785551054582010"}}},"source":["#En este modelo, siempre usar 0 es lo correcto. Y después de entrenar efectivamente solo da 1, Victoria!\n","obs = env.reset()\n","action, _ = model.action_value(obs[None])\n","print(action)"],"execution_count":108,"outputs":[{"output_type":"stream","text":["tf.Tensor([0], shape=(1,), dtype=int64)\n"],"name":"stdout"}]}]}