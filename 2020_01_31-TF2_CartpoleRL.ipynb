{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2020_01_31-TF2_CartpoleRL.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPfYqK6QZnTzLmVaECEArk5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"X0TJ_6SU7UTn","colab_type":"code","colab":{}},"source":["#http://inoryy.com/post/tensorflow2-deep-reinforcement-learning/"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MNsgw66t7dmF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"5ba77a67-7e17-4362-d1d0-81a90066d8cf","executionInfo":{"status":"ok","timestamp":1581014055833,"user_tz":360,"elapsed":10613,"user":{"displayName":"Cesar vega","photoUrl":"","userId":"04805785551054582010"}}},"source":["from __future__ import absolute_import, division, print_function, unicode_literals\n","\n","# TensorFlow and tf.keras\n","try:\n","  # %tensorflow_version only exists in Colab.\n","  %tensorflow_version 2.x\n","except Exception:\n","  pass\n","import tensorflow as tf\n","from tensorflow import keras\n","import numpy as np"],"execution_count":2,"outputs":[{"output_type":"stream","text":["TensorFlow 2.x selected.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ffdo8_HB7pYw","colab_type":"code","colab":{}},"source":["import gym\n","import logging\n","import matplotlib.pyplot as plt\n","import tensorflow.keras.layers as kl\n","import tensorflow.keras.losses as kls\n","import tensorflow.keras.optimizers as ko"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mFDBv1RB8Is6","colab_type":"text"},"source":["#Reinforcement Learning theory\n","Generally speaking, reinforcement learning is a high-level framework for solving sequential decision-making problems. An RL agent navigates an environment by taking actions based on some observations, receiving rewards as a result. Most RL algorithms work by maximizing the expected total rewards an agent collects in a trajectory, e.g., during one in-game round.\n","\n","The output of an RL algorithm is a policy – a function from states to actions.\n","A valid policy can be as simple as a hard-coded no-op action, but typically it represents a conditional probability distribution of actions given some state.\n","\n","RL algorithms are often grouped based on their optimization loss function.\n","\n","<ul>\n","  <li>\n","  Temporal-Difference methods, such as Q-Learning, reduce the error between predicted and actual state(-action) values.\n","  </li>\n","    <li>\n","  Policy Gradients directly optimize the policy by adjusting its parameters. Calculating gradients themselves is usually infeasible; instead, they are often estimated via monte-carlo methods.\n","  </li>\n","  <li>\n","  The most popular approach is a hybrid of the two: actor-critic methods, where policy gradients optimize agent’s policy, and the temporal-difference method is used as a bootstrap for the expected value estimates.\n","  </li>\n","\n","\n","\n","</ul>"]},{"cell_type":"markdown","metadata":{"id":"x_Le2SIl8lue","colab_type":"text"},"source":["#Deep Reinforcement Learning theory\n","\n","An RL algorithm is considered deep if the policy and value functions are approximated with neural networks.\n","\n","<ul>\n","  <li>\n","  (Asynchronous) Advantage Actor-Critic:\n","\n","  1. First, gradients are weighted with returns: a discounted sum of future rewards, which resolves theoretical issues with infinite timesteps, and mitigaes the \"credit assignment problem\" - allocate rewards to the correct actions\n","  2.An advantage function is used instead of raw returns. Advantage is formed as the difference between returns and some baseline, which is often the value estimate;\n","```\n"," Advantage = returns - baseline\n","```\n","and can be tought of as a measure of how good a given action is compared to some average.\n","  \n","  3. An additional entropy maximization term is used in the objective function to ensure the agent sufficiently explores various policies. In essence entropy measures how random a given probability distribution is. (For example, entropy is highest in the uniform distribution.)\n","\n","  4. Finally multiple workers are used in parallel to speed up sample gathering while helping decorrelate them during training, diversifying the experiences an agent trains on in a given batch.\n","  </li>\n","</ul>"]},{"cell_type":"markdown","metadata":{"id":"bcI9DIxx9J7-","colab_type":"text"},"source":["#Advantage Actor-Critic With TensorFlow 2.1\n","\n","Now that we are more or less on the same page, let’s see what it takes to implement the basis of many modern DRL algorithms: an actor-critic agent, described in the previous section. Without parallel workers (for simplicity), though most of the code would be the same."]},{"cell_type":"markdown","metadata":{"id":"MRIObGW-9NHW","colab_type":"text"},"source":["We use the Cartpole-v0 environment as a testbed."]},{"cell_type":"markdown","metadata":{"id":"6qXKZFD99bnA","colab_type":"text"},"source":["#First, we create the policy and value estimate NNs under a single model class:"]},{"cell_type":"code","metadata":{"id":"GxFj-SyU9JYW","colab_type":"code","colab":{}},"source":["class ProbabilityDistribution(tf.keras.Model):\n","  def call(self, logits, **kwargs):\n","    # Sample a random categorical action from the given logits.\n","    return tf.squeeze(tf.random.categorical(logits,1), axis = -1)\n","    #tf squeeze: https://www.tensorflow.org/api_docs/python/tf/squeeze\n","      #returns a tensor of the same type with all dimensions of size 1 removed.\n","    #tf random categorical: https://www.tensorflow.org/api_docs/python/tf/random/categorical\n","      #RETURNS The drawn samples of shape [batch_size, num_samples].\n","      #I think you give it probabilities and size and it returns a batch of random samples from there."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lSS6JrCv8FkT","colab_type":"code","colab":{}},"source":["class Model(tf.keras.Model): #what the model is: https://www.tensorflow.org/api_docs/python/tf/keras/Model\n","   #I think this gets tf.keras.Model and modifies it\n","  def __init__(self, num_actions):\n","    super().__init__(\"mlp_policy\") #mlp_policy is probably just a name\n","    self.hidden1 = kl.Dense(128, activation = 'relu') #I dont think we are adding layers, but creating it?\n","    self.hidden2 = kl.Dense(64, activation= 'relu')\n","    self.value = kl.Dense(1, name = \"value\") #the 1 is the dimensionality of the output\n","    # Logits are unnormalized log probabilities.\n","    self.logits = kl.Dense(num_actions, name = \"policy_logits\")\n","    self.dist = ProbabilityDistribution()\n","  \n","  def call(self,inputs, **kwargs):\n","    # Inputs is a numpy array, convert to a tensor.\n","    print(\"Call start \",\"inputs: \", inputs)\n","    x = tf.convert_to_tensor(inputs)\n","    print(\"x converted to tensor: \", x)\n","    hidden_logs = self.hidden1(x) #This are two different networks?\n","    print(\"got hidden logs, \", hidden_logs)\n","    hidden_vals = self.hidden2(x)\n","    print(\"got hidden vals, \", hidden_vals)\n","    print(\"The logits are: \", self.logits(hidden_logs))\n","    print(\"The values are: \", self.value(hidden_vals))\n","\n","    return self.logits(hidden_logs), self.value(hidden_vals)\n","  \n","  def action_value(self,obs): #Le estamos pasando solo la obs y las clases ya definidas\n","    # Executes `call()` under the hood.\n","    print(\"Action\")\n","    logits, value = self.predict_on_batch(obs) \n","    print(\"Back to action_value\")\n","    print(\"After predict_on_batch: got logits\", logits)\n","    print(\"After predict_on_batch: got value\", value)\n","    action = self.dist.predict_on_batch(logits) #dist es probability distribution. You get it from the logits\n","    return np.squeeze(action, axis=-1), np.squeeze(value, axis=-1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BKqXXlJ1BKIU","colab_type":"text"},"source":["And verify the model works as expected:\n","(plus some experiments)"]},{"cell_type":"code","metadata":{"id":"DZrdzsJiBKvt","colab_type":"code","colab":{}},"source":["#simply making the environment\n","env = gym.make('CartPole-v0')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aBlqVvP4CDvB","colab_type":"code","outputId":"d7fd0514-871d-4965-df43-1aa9059db23b","executionInfo":{"status":"ok","timestamp":1581014055841,"user_tz":360,"elapsed":10598,"user":{"displayName":"Cesar vega","photoUrl":"","userId":"04805785551054582010"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["#gets the action space from the environment\n","print(env.action_space)\n","print(env.action_space.n)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Discrete(2)\n","2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Fvtj0AIPBVg1","colab_type":"code","outputId":"109f2282-f5cd-45f7-9a68-f71c0f056110","executionInfo":{"status":"ok","timestamp":1581014055842,"user_tz":360,"elapsed":10591,"user":{"displayName":"Cesar vega","photoUrl":"","userId":"04805785551054582010"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["#gets the model we made, \n","model = Model(num_actions=env.action_space.n)\n","model"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<__main__.Model at 0x7f9d185d7e48>"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"wrARq5VlCA_r","colab_type":"code","outputId":"624e556c-0c71-4bf1-c611-bb2e65975565","executionInfo":{"status":"ok","timestamp":1581014055843,"user_tz":360,"elapsed":10585,"user":{"displayName":"Cesar vega","photoUrl":"","userId":"04805785551054582010"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["#Gets the observation\n","obs = env.reset()\n","print(obs.shape)\n","obs"],"execution_count":9,"outputs":[{"output_type":"stream","text":["(4,)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["array([-0.02788602,  0.00177448,  0.04136009, -0.0131044 ])"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"QHhTyLhvGzKQ","colab_type":"code","outputId":"ff6625b1-1265-430c-afa4-e9766aa42b3c","executionInfo":{"status":"ok","timestamp":1581014055844,"user_tz":360,"elapsed":10579,"user":{"displayName":"Cesar vega","photoUrl":"","userId":"04805785551054582010"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["print(obs[None, :].shape) #la forma de este es (1,4), en vez de (4,)\n","#Returns an array but it has double brackets,\n","obs[None] #I think its like adding the required batch\n","#Note obs[None] is the same as obs[None:]"],"execution_count":10,"outputs":[{"output_type":"stream","text":["(1, 4)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["array([[-0.02788602,  0.00177448,  0.04136009, -0.0131044 ]])"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"8lXJ3RA-uqae","colab_type":"text"},"source":["Lo que pasa cuando usas model.action_value:\n","\n","1. Se abre comienza a ejecutar la definción action_value.. ,que es self.predict_on_batch(), eso lo pasa a call()\n","\n","2. Toda la función call() se ejecuta antes de seguir con action_value\n","<ul>\n","  <li>\n","    1. se transforma el input a un tensor. Cabe decir que call corre dos veces, al parecer hay dos inputs. Uno de shape (None, 4) otro de shape (1,4). El shape (1,4) es el obs[None, :]. El (None, 4) no estoy 100% seguro, PERO creo que son las 4 frames que juntamos. Como llegaron ahí, no lo se. Es posible que sean parte de obs que no se como ver.\n","  </li>\n","    <li>\n","    2. El input (None,4) , ya transformado se pasa a las redes neuronales hidden1 y hidden2, una te regresa logits, shape = (None,2) y el otro te regresa values shape = (None, 1). Estas shapes son así porque en la clase le indicamos para hidden1 que la red final es una Dense con un output de num_actions, y para hidden2 es un output de Dense(1)\n","  </li>\n","  <li>\n","    3. Al parecer estos datos los regresa, (porque al final de la función tiene un return()) pero no se donde se usan despues.\n","  </li>\n","  <li>\n","    4. Vuelve a empezar el call(), pero esta vez su input es shape (1,4), pasan los mismos pasos, pero ahora nuestros outputs de las Dense, tienen shape (1,2) y (1,1) que son lo mismo, pero sin el None al inicio.\n","  </li>\n","</ul>\n","\n","3. Ahora regresamos a action_value()\n","<ul>\n","  <li>\n","    1. Usa predict_on_batch con obs (que es obs[None, :] de input, creo que como el modelo tiene dos redes y dos outputs, predict on batch te regresa dos valores, el value y el logits. (Voy a hacer un experimento, en el que modificaré una clase de red a ver si puedo hacer que me de outputs o cosas rarash (mas outputs, un input, cosas así) )\n","  </li>\n","</ul>"]},{"cell_type":"code","metadata":{"id":"CJtrmgWdF5zy","colab_type":"code","outputId":"03acfae1-8aa2-4bb3-9932-9eb0b49146b8","executionInfo":{"status":"ok","timestamp":1581014067036,"user_tz":360,"elapsed":21763,"user":{"displayName":"Cesar vega","photoUrl":"","userId":"04805785551054582010"}},"colab":{"base_uri":"https://localhost:8080/","height":326}},"source":["#returns the action we take (from the two availiable), pretty simple, either 1 or 0\n","#And the value, but what does the value mean?\n","action, value = model.action_value(obs[None, :])\n","print(action, value) # [1] [-0.00145713]"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Action\n","Call start  inputs:  Tensor(\"input_1:0\", shape=(None, 4), dtype=float32)\n","x converted to tensor:  Tensor(\"input_1:0\", shape=(None, 4), dtype=float32)\n","got hidden logs,  Tensor(\"model/dense/Identity:0\", shape=(None, 128), dtype=float32)\n","got hidden vals,  Tensor(\"model/dense_1/Identity:0\", shape=(None, 64), dtype=float32)\n","The logits are:  Tensor(\"model/policy_logits/Identity:0\", shape=(None, 2), dtype=float32)\n","The values are:  Tensor(\"model/value/Identity:0\", shape=(None, 1), dtype=float32)\n","Call start  inputs:  Tensor(\"self:0\", shape=(1, 4), dtype=float32)\n","x converted to tensor:  Tensor(\"self:0\", shape=(1, 4), dtype=float32)\n","got hidden logs,  Tensor(\"model/dense/Relu:0\", shape=(1, 128), dtype=float32)\n","got hidden vals,  Tensor(\"model/dense_1/Relu:0\", shape=(1, 64), dtype=float32)\n","The logits are:  Tensor(\"model/policy_logits/BiasAdd:0\", shape=(1, 2), dtype=float32)\n","The values are:  Tensor(\"model/value/BiasAdd:0\", shape=(1, 1), dtype=float32)\n","Back to action_value\n","After predict_on_batch: got logits tf.Tensor([[ 0.01055941 -0.01101444]], shape=(1, 2), dtype=float32)\n","After predict_on_batch: got value tf.Tensor([[-0.00238202]], shape=(1, 1), dtype=float32)\n","0 [-0.00238202]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"T8FeFsuy99vE","colab_type":"text"},"source":["#Esto es un espacio de experimentación, en el que modificaré una clase de un modelo."]},{"cell_type":"markdown","metadata":{"id":"Had49D8gCY5w","colab_type":"text"},"source":["Un modelo como el siguiente, lo dejé solo con una sola red, y un solo output. Y por lo tanto me da un solo valor de action_value"]},{"cell_type":"code","metadata":{"id":"o7th73bzCwoN","colab_type":"code","colab":{}},"source":["class ex2Model(tf.keras.Model): #what the model is: https://www.tensorflow.org/api_docs/python/tf/keras/Model\n","   #I think this gets tf.keras.Model and modifies it\n","  def __init__(self, num_actions):\n","    super().__init__() #mlp_policy is probably just a name\n","    print(\"__init__ started\")\n","    self.hidden1 = kl.Dense(128, activation = 'relu') #I dont think we are adding layers, but creating it?\n","    self.value = kl.Dense(1, name = \"value\") #the 1 is the dimensionality of the output\n","    print(\"__init__ finished\")\n","\n","  def call(self,inputs, **kwargs): #the call function is something you change from the model\n","    # Inputs is a numpy array, convert to a tensor.\n","    print(\"call inputs: \", inputs)\n","    x = tf.convert_to_tensor(inputs)\n","    print(\"Converted inputs to tensor \", x)\n","    xThroughHidden1 = self.hidden1(x) #This are two different networks?\n","    print(\"passed tensorized inputs through the hidden layer \",xThroughHidden1)\n","    print(\"pass the results of the hidden layer 1, through a dense layer with output 1: \", self.value(xThroughHidden1) )\n","    return self.value(xThroughHidden1) \n","  \n","  def action_value(self,obs): #Le estamos pasando solo la obs y las clases ya definidas\n","    # Executes `call()` under the hood.\n","    print(\"Enter action_value\")\n","    value = self.predict_on_batch(obs)  #predict on batch, basically runs call()\n","    print(\"The value before squeeze: \", value.shape) \n","    return np.squeeze(value, axis=-1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GxNqmvyLB8b5","colab_type":"code","outputId":"efd900e3-315a-4109-c7b1-42bbce7d93a1","executionInfo":{"status":"ok","timestamp":1581014067332,"user_tz":360,"elapsed":22049,"user":{"displayName":"Cesar vega","photoUrl":"","userId":"04805785551054582010"}},"colab":{"base_uri":"https://localhost:8080/","height":326}},"source":["Emodel = ex2Model(num_actions=env.action_space.n)\n","enterData = np.zeros((4,22,8))\n","print(enterData.shape)\n","print(Emodel.action_value(enterData))"],"execution_count":13,"outputs":[{"output_type":"stream","text":["__init__ started\n","__init__ finished\n","(4, 22, 8)\n","Enter action_value\n","call inputs:  Tensor(\"input_1_2:0\", shape=(None, 22, 8), dtype=float32)\n","Converted inputs to tensor  Tensor(\"input_1_2:0\", shape=(None, 22, 8), dtype=float32)\n","passed tensorized inputs through the hidden layer  Tensor(\"ex2_model/dense_2/Identity:0\", shape=(None, 22, 128), dtype=float32)\n","pass the results of the hidden layer 1, through a dense layer with output 1:  Tensor(\"ex2_model/value/Identity:0\", shape=(None, 22, 1), dtype=float32)\n","call inputs:  Tensor(\"self:0\", shape=(4, 22, 8), dtype=float32)\n","Converted inputs to tensor  Tensor(\"self:0\", shape=(4, 22, 8), dtype=float32)\n","passed tensorized inputs through the hidden layer  Tensor(\"ex2_model/dense_2/Relu:0\", shape=(4, 22, 128), dtype=float32)\n","pass the results of the hidden layer 1, through a dense layer with output 1:  Tensor(\"ex2_model/value/BiasAdd:0\", shape=(4, 22, 1), dtype=float32)\n","The value before squeeze:  (4, 22, 1)\n","[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"p_fb8JZUFFyh","colab_type":"text"},"source":["Una pequeña e interesante revelación. Sea como sea el input (Cualquier shape) siempre call() corre dos veces, Si tu shape es (Y,N,Z) corre una vez con shape (None,N,Z) la otra con (Y,N,Z).\n","\n","PERO la primera vez que corre no te regresa \"un valor\", te regresa un tensor de shape shape=(None, N, DenseOutputShape) pero \"sin nada adentro??\". POR otro lado la segunda ya te regresa algo con la shape (Y,N,DenseOutputShape). Esto creo que es interesante porque podrías hacer Y*N*DenseOutputShape y ahora tienes todo unidimensional"]},{"cell_type":"markdown","metadata":{"id":"IijxRwSOm74f","colab_type":"text"},"source":["#Regresando al la función original\n","\n","Basicamente call(), nos regresa dos valores del mismo input, uno de shape num_actions y otro de shape 1.\n","\n","La situación es que despues del primer predict_on_batch, hay otro, pero este otro tiene un .dist antes (que es un ProbabilityDistribution).\n","\n","Nota que en el second predict_on_batch, no pasa por call(), porque no se imprime todo"]},{"cell_type":"code","metadata":{"id":"Vi7pSKT9EvuK","colab_type":"code","colab":{}},"source":["class Model(tf.keras.Model): #what the model is: https://www.tensorflow.org/api_docs/python/tf/keras/Model\n","   #I think this gets tf.keras.Model and modifies it\n","  def __init__(self, num_actions):\n","    super().__init__(\"mlp_policy\") #mlp_policy is probably just a name\n","    self.hidden1 = kl.Dense(128, activation = 'relu') #I dont think we are adding layers, but creating it?\n","    self.hidden2 = kl.Dense(64, activation= 'relu')\n","    self.value = kl.Dense(1, name = \"value\") #the 1 is the dimensionality of the output\n","    # Logits are unnormalized log probabilities.\n","    self.logits = kl.Dense(num_actions, name = \"policy_logits\")\n","    self.dist = ProbabilityDistribution()\n","  \n","  def call(self,inputs, **kwargs):\n","    # Inputs is a numpy array, convert to a tensor.\n","    #print(\"Call start \",\"inputs: \", inputs)\n","    x = tf.convert_to_tensor(inputs)\n","    #print(\"x converted to tensor: \", x)\n","    hidden_logs = self.hidden1(x) #This are two different networks?\n","    #print(\"got hidden logs, \", hidden_logs)\n","    hidden_vals = self.hidden2(x)\n","    #print(\"got hidden vals, \", hidden_vals)\n","    #print(\"The logits are: \", self.logits(hidden_logs))\n","    #print(\"The values are: \", self.value(hidden_vals))\n","\n","    return self.logits(hidden_logs), self.value(hidden_vals)\n","  \n","  def action_value(self,obs): #Le estamos pasando solo la obs y las clases ya definidas\n","    # Executes `call()` under the hood.\n","    #print(\"Action\")\n","    logits, value = self.predict_on_batch(obs) \n","    #print(\"Back to action_value\")\n","    #print(\"After first predict_on_batch: got logits\", logits)\n","    #print(\"After first predict_on_batch: got value\", value) \n","    #Nota que en el second predict_on_batch, no pasa por call()\n","    #Como que le estamos diciendo que haga predict on batch pero de la distribución.\n","    action = self.dist.predict_on_batch(logits) #dist es probability distribution. You get it from the logits\n","    #print(\"After second predict on batch, the action array is: \", action)\n","    return np.squeeze(action, axis=-1), np.squeeze(value, axis=-1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qHG0mXIs_5e4","colab_type":"code","outputId":"3334097f-bebf-4111-9ba9-cabf1a4e7be1","executionInfo":{"status":"ok","timestamp":1581014067334,"user_tz":360,"elapsed":22042,"user":{"displayName":"Cesar vega","photoUrl":"","userId":"04805785551054582010"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["env = gym.make('CartPole-v0')\n","model = Model(num_actions=env.action_space.n)\n","\n","obs = env.reset()\n","print(obs.shape)\n","# No feed_dict or tf.Session() needed at all!\n","action, value = model.action_value(obs[None, :])\n","print(action, value)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["(4,)\n","1 [0.00621449]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hpmb6YgAv_IO","colab_type":"text"},"source":["#Como funciona la función:\n","1. La función toma como base (tf.keras.Model) y a partir de ahí modificas lo que sea necesario.\n","\n","2. en __init__():\n","<ul>\n","  <li>\n","  lo que haces es indicar, que al inicializar en modelo necesitas pasarle el num_actions\n","  </li>\n","  <li>\n","    Y aparte defines funciones dentro de la clase, en este caso definimos dos redes (independientes) hidden1 y hidden2,(con pesos independientes.). También definimos otras dos redes de salida, value y logits. Así como una ProbabilityDistribution() function.\n","  </li>\n","</ul> \n","\n","3. En call(), lo que pasa es que al correr self.predict_on_batch() corre call(). call() lo que hace es:\n","<ul>\n","  <li>\n","  Tomar el array que le damos como input y transformarlo a un tensor. Luego pasa este input (independientemente) por cada red. (A hidden1 y hidden2 le pasamos el mismo input pero nos da diferente valor). y los valores que saca de una red (hidden1) la pasa a logits (que es una Dense con output de num_action) y los valores de otra red (hidden2) los pasa a Value (que es una Dense con output 1)\n","  </li>\n","  <li>\n","    Nota: en call corre dos veces la primera vez que lo llamas, en esta primera vez corre con el array pero la shape la deja como (None, N), en vez de tu shape original, luego ya lo corre con tu shape original (Y todas las funciones que corran en call() después no te hacen esto de correr con (None)\n","  </li>\n","</ul> \n","\n","4. Al final esta action_value, que lo primero que hace es correr call(). Y luego usa la probability distribution con uno de los valores que regresa call para sacar que acción usamos."]},{"cell_type":"markdown","metadata":{"id":"1q_NxKzZyhEP","colab_type":"text"},"source":["#Agent Interface\n","\n","Now we can move on to the fun stuff – the agent class. First, we add a test method that runs through a full episode, keeping track of the rewards."]},{"cell_type":"code","metadata":{"id":"0tYf28lOt73r","colab_type":"code","colab":{}},"source":["class A2CAgent:\n","  def __init__(self, model): #literal solo le pasas el modelo que hicimos antes\n","    self.model = model\n","\n","  def test(self, env, render=True):\n","    obs, done, ep_reward = env.reset(), False, 0 #Get the initial values from the environment\n","    while not done:\n","      action, _ = self.model.action_value(obs[None, :]) #returns the action the model recomends and a value? (What is the value for?)\n","      obs, reward, done, _ = env.step(action)\n","      ep_reward += reward #get the reward of the action\n","      #if render: #I think this is for visualization purposes.\n","       # env.render()\n","    return ep_reward"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P_n8RVyKzXXN","colab_type":"text"},"source":["Now we can check how much the agent scores with randomly initialized weights:"]},{"cell_type":"code","metadata":{"id":"wNqd8T-qzqpG","colab_type":"code","colab":{}},"source":["model = Model(num_actions=env.action_space.n)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WEwKtgvZyjt2","colab_type":"code","outputId":"04b4d809-dec4-4ae6-934b-93c1d43c1e21","executionInfo":{"status":"ok","timestamp":1581014067658,"user_tz":360,"elapsed":22356,"user":{"displayName":"Cesar vega","photoUrl":"","userId":"04805785551054582010"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["agent = A2CAgent(model)\n","rewards_sum = agent.test(env)\n","print(\"%d out of 200\" % rewards_sum)"],"execution_count":18,"outputs":[{"output_type":"stream","text":["16 out of 200\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ifNO8zdc0vsM","colab_type":"text"},"source":["#The training"]},{"cell_type":"markdown","metadata":{"id":"2vKlHI7E0yT6","colab_type":"text"},"source":["#Loss / Objective Function and The training loop\n","an agent improves its policy through gradient descent based on some loss (objective) function. In the A2C algorithm, we train on three objectives: improve policy with advantage weighted gradients, maximize the entropy, and minimize value estimate errors."]},{"cell_type":"code","metadata":{"id":"sKv1mS5D0vZU","colab_type":"code","colab":{}},"source":["import tensorflow.keras.losses as kls\n","import tensorflow.keras.optimizers as ko\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z_kpN6QTU1uR","colab_type":"text"},"source":["Get the full agent class"]},{"cell_type":"code","metadata":{"id":"Z4t0bfZ-zbzF","colab_type":"code","colab":{}},"source":["class A2CAgent:\n","  def __init__(self, model, lr=7e-3, gamma=0.99, value_c=0.5, entropy_c=1e-4):\n","    # Coefficients are used for the loss terms.\n","    self.value_c = value_c\n","    self.entropy_c = entropy_c\n","    self.gamma = gamma\n","\n","    self.model = model\n","    self.model.compile(\n","      optimizer=ko.RMSprop(lr=lr),\n","      # Define separate losses for policy logits and value estimate.\n","      loss=[self._logits_loss, self._value_loss])\n","\n","  def test(self, env, render=True):\n","    obs, done, ep_reward = env.reset(), False, 0 #Get the initial values from the environment\n","    while not done:\n","      action, _ = self.model.action_value(obs[None, :]) #returns the action the model recomends and a value? (What is the value for?)\n","      obs, reward, done, _ = env.step(action)\n","      ep_reward += reward #get the reward of the action\n","      #if render: #I think this is for visualization purposes.\n","       # env.render()\n","    return ep_reward\n","\n","  def _value_loss(self, returns, value):\n","    # Value loss is typically MSE between value estimates and returns.\n","    return self.value_c * kls.mean_squared_error(returns, value)\n","\n","  def _logits_loss(self, actions_and_advantages, logits):\n","    # A trick to input actions and advantages through the same API.\n","    # split that value into 2 ¿through the -1 axis?\n","    actions, advantages = tf.split(actions_and_advantages, 2, axis=-1)\n","\n","    # Sparse categorical CE loss obj that supports sample_weight arg on `call()`.\n","    # `from_logits` argument ensures transformation into normalized probabilities.\n","    weighted_sparse_ce = kls.SparseCategoricalCrossentropy(from_logits=True)\n","\n","    # Policy loss is defined by policy gradients, weighted by advantages.\n","    # Note: we only calculate the loss on the actions we've actually taken.\n","    actions = tf.cast(actions, tf.int32) #transforms into shape tf.int32\n","    policy_loss = weighted_sparse_ce(actions, logits, sample_weight=advantages)\n","\n","    # Entropy loss can be calculated as cross-entropy over itself.\n","    probs = tf.nn.softmax(logits)\n","    entropy_loss = kls.categorical_crossentropy(probs, probs)\n","\n","    # We want to minimize policy and maximize entropy losses.\n","    # Here signs are flipped because the optimizer minimizes.\n","    return policy_loss - self.entropy_c * entropy_loss\n","\n","  def _returns_advantages(self, rewards, dones, values, next_value):\n","    # `next_value` is the bootstrap value estimate of the future state (critic).\n","    #append next_value to np_zeroes_like(rewards) through the -1 axis, and that is returns\n","    returns = np.append(np.zeros_like(rewards), next_value, axis=-1)\n","\n","    # Returns are calculated as discounted sum of future rewards.\n","    for t in reversed(range(rewards.shape[0])): \n","      #bellmans equation\n","      returns[t] = rewards[t] + self.gamma * returns[t + 1] * (1 - dones[t])\n","    returns = returns[:-1] #Take just a single value\n","\n","    # Advantages are equal to returns - baseline (value estimates in our case).\n","    advantages = returns - values\n","\n","    return returns, advantages\n","\n","  def train(self, env, batch_sz=64, updates=250):\n","    # Storage helpers for a single batch of data.\n","    actions = np.empty((batch_sz,), dtype=np.int32)\n","    rewards, dones, values = np.empty((3, batch_sz)) #Create 3 empty arrays of batch_size:\n","    observations = np.empty((batch_sz,) + env.observation_space.shape) #create an array of (batchsize, observation_space.shape)\n","\n","    # Training loop: collect samples, send to optimizer, repeat updates times.\n","    ep_rewards = [0.0] #current episode rewards\n","    next_obs = env.reset() #get observation from the environment\n","    for update in range(updates): #updates is how many times we play the game (our batch count)\n","      for step in range(batch_sz): #run 64 frames to get the batch\n","        observations[step] = next_obs.copy() #set the next_obs in an array of observation\n","        actions[step], values[step] = self.model.action_value(next_obs[None, :]) #pass the obs into the action of our model (the two nnets)\n","        next_obs, rewards[step], dones[step], _ = env.step(actions[step]) #get the next state of the nevironment with the action taken\n","\n","        ep_rewards[-1] += rewards[step]\n","        if dones[step]:  #if we win or loose.\n","          ep_rewards.append(0.0) #start new rewards\n","          next_obs = env.reset() #start new environment\n","          logging.info(\"Episode: %03d, Reward: %03d\" % (\n","            len(ep_rewards) - 1, ep_rewards[-2]))\n","\n","      _, next_value = self.model.action_value(next_obs[None, :]) #after a single batch is finished, run another one before starting the new loop\n","\n","      returns, advs = self._returns_advantages(rewards, dones, values, next_value)\n","      # A trick to input actions and advantages through same API.\n","      acts_and_advs = np.concatenate([actions[:, None], advs[:, None]], axis=-1)\n","\n","      # Performs a full training step on the collected batch.\n","      # Note: no need to mess around with gradients, Keras API handles it.\n","      #observations is the training data, [...the other array...] is the target data.\n","      losses = self.model.train_on_batch(observations, [acts_and_advs, returns])\n","\n","      logging.debug(\"[%d/%d] Losses: %s\" % (update + 1, updates, losses))\n","\n","    return ep_rewards\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gXbj16DlbeBh","colab_type":"text"},"source":["#Results\n","\n","We are now all set to train our single-worker A2C agent on CartPole-v0! The training process should take a couple of minutes. After the training is complete, you should see an agent achieve the target 200 out of 200 score."]},{"cell_type":"code","metadata":{"id":"EFeo2tXIaNFj","colab_type":"code","outputId":"ed76acd9-e81c-40af-a493-17b39e9c43ac","executionInfo":{"status":"ok","timestamp":1581014098806,"user_tz":360,"elapsed":53493,"user":{"displayName":"Cesar vega","photoUrl":"","userId":"04805785551054582010"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["agent = A2CAgent(model)\n","rewards_history = agent.train(env)\n","print(\"Finished training, testing...\")\n","print(\"%d out of 200\" % agent.test(env)) # 200 out of 200"],"execution_count":21,"outputs":[{"output_type":"stream","text":["Finished training, testing...\n","122 out of 200\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"flnC9uQQmXfF","colab_type":"text"},"source":["#Experimenting with the agent class\n","\n","Since the agent class is a giantic beast and a mess. I will see how it works"]},{"cell_type":"code","metadata":{"id":"GRIU9ZWDbhLS","colab_type":"code","colab":{}},"source":["class A2CAgentExperiment:\n","  def __init__(self, model, lr=7e-3, gamma=0.99, value_c=0.5, entropy_c=1e-4):\n","    print(\"Enter __init__\")\n","    # Coefficients are used for the loss terms.\n","    self.value_c = value_c\n","    self.entropy_c = entropy_c\n","    self.gamma = gamma\n","\n","    self.model = model\n","    print(\"before __init__ compile\")\n","    self.model.compile(\n","      optimizer=ko.RMSprop(lr=lr),\n","      # Define separate losses for policy logits and value estimate.\n","      loss=[self._logits_loss, self._value_loss])\n","\n","    print(\"end of __init__\")\n","\n","  def test(self, env, render=True):\n","    obs, done, ep_reward = env.reset(), False, 0 #Get the initial values from the environment\n","    while not done:\n","      action, _ = self.model.action_value(obs[None, :]) #returns the action the model recomends and a value? (What is the value for?)\n","      obs, reward, done, _ = env.step(action)\n","      ep_reward += reward #get the reward of the action\n","      #if render: #I think this is for visualization purposes.\n","       # env.render()\n","    return ep_reward\n","\n","  def _value_loss(self, returns1, value1):\n","    print(\"Value loss enter, Returns: \", kls.mean_squared_error(returns1, value1))\n","    # Value loss is typically MSE between value estimates and returns.\n","    return self.value_c * kls.mean_squared_error(returns1, value1)\n","\n","  def _logits_loss(self, actions_and_advantages, logits):\n","    print(\"_logits_loss enter\")\n","    # A trick to input actions and advantages through the same API.\n","    # split that value into 2 ¿through the -1 axis?\n","    actions, advantages = tf.split(actions_and_advantages, 2, axis=-1)\n","\n","    # Sparse categorical CE loss obj that supports sample_weight arg on `call()`.\n","    # `from_logits` argument ensures transformation into normalized probabilities.\n","    weighted_sparse_ce = kls.SparseCategoricalCrossentropy(from_logits=True)\n","\n","    # Policy loss is defined by policy gradients, weighted by advantages.\n","    # Note: we only calculate the loss on the actions we've actually taken.\n","    actions = tf.cast(actions, tf.int32) #transforms into shape tf.int32\n","    policy_loss = weighted_sparse_ce(actions, logits, sample_weight=advantages)\n","\n","    # Entropy loss can be calculated as cross-entropy over itself.\n","    probs = tf.nn.softmax(logits)\n","    entropy_loss = kls.categorical_crossentropy(probs, probs)\n","\n","    # We want to minimize policy and maximize entropy losses.\n","    # Here signs are flipped because the optimizer minimizes.\n","    return policy_loss - self.entropy_c * entropy_loss\n","\n","  def _returns_advantages(self, rewards, dones, values, next_value):\n","    print(\"_returns_advantages enter\")\n","    # `next_value` is the bootstrap value estimate of the future state (critic).\n","    #append next_value to np_zeroes_like(rewards) through the -1 axis, and that is returns\n","    returns = np.append(np.zeros_like(rewards), next_value, axis=-1)\n","    # Returns are calculated as discounted sum of future rewards.\n","    #print(\"rewards.shape[0] \", rewards.shape[0])\n","    for t in reversed(range(rewards.shape[0])):\n","      #bellmans equation\n","      returns[t] = rewards[t] + self.gamma * returns[t + 1] * (1 - dones[t])\n","    returns = returns[:-1] #Take only 64 values\n","    \n","    #print(\"Bellmans result is returns: \", returns, \" the values are taken from the nnets, values: \", values)\n","\n","    # Advantages are equal to returns - baseline (value estimates in our case).\n","    advantages = returns - values\n","\n","    return returns, advantages\n","\n","  def train(self, env, batch_sz=64, updates=250):\n","    print(\"train enter\")\n","    # Storage helpers for a single batch of data.\n","    actions = np.empty((batch_sz,), dtype=np.int32)\n","    rewards, dones, values = np.empty((3, batch_sz)) #Create 3 empty arrays of batch_size:\n","    observations = np.empty((batch_sz,) + env.observation_space.shape) #create an array of (batchsize, observation_space.shape)\n","    print(\"create empty arrays for actions, rewards, dones, values, and observations\")\n","    print(actions.shape, \" \", rewards.shape, \" \", dones.shape, \" \", values.shape, \" \", observations.shape)\n","\n","    # Training loop: collect samples, send to optimizer, repeat updates times.\n","    ep_rewards = [0.0] #current episode rewards\n","    next_obs = env.reset() #get observation from the environment\n","    for update in range(updates): #updates is how many times we play the game (our batch count)\n","      for step in range(batch_sz): #run 64 frames to get the batch\n","        observations[step] = next_obs.copy() #set the next_obs in an array of observation\n","        actions[step], values[step] = self.model.action_value(next_obs[None, :]) #pass the obs into the action of our model (the two nnets)\n","        next_obs, rewards[step], dones[step], _ = env.step(actions[step]) #get the next state of the nevironment with the action taken\n","\n","        ep_rewards[-1] += rewards[step]\n","        if dones[step]:  #if we win or loose.\n","          ep_rewards.append(0.0) #start new rewards\n","          next_obs = env.reset() #start new environment\n","          logging.info(\"Episode: %03d, Reward: %03d\" % (\n","            len(ep_rewards) - 1, ep_rewards[-2]))\n","\n","      #Actually, this next_value is to calculate bellmans,\n","      _, next_value = self.model.action_value(next_obs[None, :]) #after a single batch is finished, run another one before starting the new loop\n","\n","      returns, advs = self._returns_advantages(rewards, dones, values, next_value) #Aqui pasas de train a returns_advantages\n","      # A trick to input actions and advantages through same API.\n","      acts_and_advs = np.concatenate([actions[:, None], advs[:, None]], axis=-1)\n","\n","      # Performs a full training step on the collected batch.\n","      # Note: no need to mess around with gradients, Keras API handles it.\n","      #observations is the training data, [...the other array...] is the target data.\n","      #Train on batch uses the loss we defined in init, which takes logit_loss and value loss.\n","      #I think it runs twice, once per nnet\n","      print(\"train_on_batch\")\n","      losses = self.model.train_on_batch(observations, [acts_and_advs, returns]) #Creo que aquí es donde entra logits loss\n","      logging.debug(\"[%d/%d] Losses: %s\" % (update + 1, updates, losses))\n","\n","    return ep_rewards\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JIXvPhWJ0P1F","colab_type":"code","outputId":"1beaf51a-dcaf-4305-9838-11a261bf5100","executionInfo":{"status":"ok","timestamp":1581014131900,"user_tz":360,"elapsed":86580,"user":{"displayName":"Cesar vega","photoUrl":"","userId":"04805785551054582010"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["ExpAgent = A2CAgentExperiment(model)\n","rewards_history = ExpAgent.train(env)\n","print(\"Finished training, testing...\")\n","print(\"%d out of 200\" % ExpAgent.test(env)) # 200 out of 200"],"execution_count":23,"outputs":[{"output_type":"stream","text":["Enter __init__\n","before __init__ compile\n","_logits_loss enter\n","Value loss enter, Returns:  Tensor(\"loss_1/output_2_loss/Mean:0\", shape=(None,), dtype=float32)\n","end of __init__\n","train enter\n","create empty arrays for actions, rewards, dones, values, and observations\n","(64,)   (64,)   (64,)   (64,)   (64, 4)\n","_returns_advantages enter\n","train_on_batch\n","_logits_loss enter\n","Value loss enter, Returns:  Tensor(\"loss/output_2_loss/Mean:0\", shape=(64,), dtype=float32)\n","_logits_loss enter\n","Value loss enter, Returns:  Tensor(\"loss/output_2_loss/Mean:0\", shape=(64,), dtype=float32)\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","_returns_advantages enter\n","train_on_batch\n","Finished training, testing...\n","177 out of 200\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"g-CrmcQqE3Dh","colab_type":"text"},"source":["#El camino de ejecución del agente\n","\n","1. __init__\n","\n","2. _logits_loss y value loss. <-Por el compile\n","\n","3. The train loop starts\n","\t<ul>\n","  <li>\n","    1. Sets up the environment and the arrays (actions, rewards, dones, values, observations), all have a shape (64,)   (64,)   (64,)   (64,)   (64, 4) [the batch size]\n","  </li>\n","  \n","\t2. Every update\n","\t\t1. there is a step loop where 64 steps (batch_size) of the game is played.\n","    \n","            *Where we call the model action_value function to get actions and values for every step in the environment - We are calling the neural net to tell us what to do, and then we call the environment to tell us what happened.\n","\n","\t\t2. After the batch is finished the action_value function from the model is called to get the next_value of the next_observation, this is because we need one more next_value because bellman asks for the gamma * returns[t+1] (the returns of the next action) * ....  If we did not have this step, we could not compute bellmans.\n","\n","\t\t3. The _returns_advantages function is called (This is the Bellmans function) with the values of the batch run.\n","\t\t\tNote: The returns are the Bellmans equation, and the values is what we got from the neural net. **I am starting to think that what we want to do is basically have a neural net that can approximate the value of the Bellman function for each action in each state, that way we can pick the best course of action**\n","\t\t\tNote2: The advantages is our \"loss\", we need to minimize the difference between the real bellmans and our values\n","\n","\t\t4. We make an array of ations taken and the advantage of every action by concatenating both the actions and the advantage arrays  (It is an array of 64,2 (two arrays of 64)).\n","\n","\t\t5. We use the keras train_on_batch function and pass it our observations, the acts_and_advs and the returns. \n","\t\t\tHere is where the loss function, that we told the model that it was the logits_loss and value_loss in __init__ is activated\n","\n","\t\t\tNOTE: The train_on_batch function is what actually changes our networks. based on the observations (which is a batch of (64,4) and the other ones.).\n","\n","      </ul>"]},{"cell_type":"code","metadata":{"id":"FlDfb6350vdb","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}